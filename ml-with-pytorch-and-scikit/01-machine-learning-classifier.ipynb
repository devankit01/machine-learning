{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet performs classification using the Perceptron model on the Iris dataset. Here's a breakdown:\n",
    "\n",
    "1. **Loading the Iris dataset**:\n",
    "   - The Iris dataset is a popular dataset for machine learning tasks. It contains measurements of iris flowers, categorized into three species.\n",
    "   - `load_iris()` function loads the Iris dataset, and `iris.data` contains the features (sepal length, sepal width, petal length, petal width), while `iris.target` contains the target labels (species).\n",
    "\n",
    "2. **Splitting the dataset**:\n",
    "   - `train_test_split()` function splits the dataset into training and testing sets.\n",
    "   - It assigns 80% of the data to training (`X_train`, `y_train`) and 20% to testing (`X_test`, `y_test`).\n",
    "\n",
    "3. **Feature scaling with StandardScaler**:\n",
    "   - `StandardScaler` is used to standardize the features by removing the mean and scaling to unit variance.\n",
    "   - `scaler.fit_transform(X_train)` computes the mean and standard deviation from the training data and then scales the training features.\n",
    "   - `scaler.transform(X_test)` applies the same transformation to the testing features using the parameters learned from the training data.\n",
    "\n",
    "4. **Initializing and training the Perceptron model**:\n",
    "   - `Perceptron` is a simple linear classifier that learns weights for each feature to make predictions.\n",
    "   - `perceptron = Perceptron(max_iter=1000, random_state=42)` initializes the Perceptron model with a maximum of 1000 iterations and a random seed for reproducibility.\n",
    "   - `perceptron.fit(X_train_scaled, y_train)` trains the Perceptron model on the scaled training data.\n",
    "\n",
    "5. **Making predictions**:\n",
    "   - `perceptron.predict(X_test_scaled)` predicts the target labels for the scaled testing data.\n",
    "\n",
    "6. **Evaluating the model**:\n",
    "   - `accuracy_score(y_test, y_pred)` calculates the accuracy of the model by comparing the predicted labels (`y_pred`) with the actual labels (`y_test`).\n",
    "   - The accuracy score is printed out as \"Accuracy\".\n",
    "\n",
    "Overall, this code demonstrates a simple workflow for training a Perceptron model on the Iris dataset, including preprocessing with feature scaling and evaluating model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6222222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron  # Import the Perceptron classifier\n",
    "from sklearn.datasets import load_iris  # Import the iris dataset\n",
    "from sklearn.model_selection import train_test_split  # Import train_test_split function\n",
    "from sklearn.metrics import accuracy_score  # Import accuracy_score function\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# X_train: Training features, X_test: Testing features, y_train: Training labels, y_test: Testing labels\n",
    "# test_size=0.2: 20% of the data will be used for testing, random_state=42: Random seed for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Initialize and train the Perceptron model\n",
    "# max_iter=1000: Maximum number of iterations to converge, random_state=42: Random seed for reproducibility\n",
    "perceptron = Perceptron(max_iter=1000, random_state=42)\n",
    "perceptron.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = perceptron.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "# Compare the predicted labels (y_pred) with the actual labels (y_test) and calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the Perceptron model\n",
    "perceptron = Perceptron(max_iter=1000, random_state=42)\n",
    "perceptron.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = perceptron.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a brief overview of Support Vector Classifier (SVC), Decision Tree Classifier, Random Forest Classifier, and K-Nearest Neighbors Classifier:\n",
    "\n",
    "1. **Support Vector Classifier (SVC)**:\n",
    "   - SVC is a supervised learning algorithm used for classification tasks.\n",
    "   - It works by finding the hyperplane that best separates the classes in the feature space.\n",
    "   - SVC can handle linear and non-linear classification tasks through the use of different kernel functions, such as linear, polynomial, and radial basis function (RBF) kernels.\n",
    "   - It is effective in high-dimensional spaces and can handle datasets with many features.\n",
    "\n",
    "2. **Decision Tree Classifier**:\n",
    "   - Decision Tree Classifier is a non-parametric supervised learning algorithm used for classification tasks.\n",
    "   - It creates a tree-like structure where each node represents a feature and each branch represents a decision based on that feature.\n",
    "   - Decision trees split the feature space into regions that are as pure as possible in terms of the target variable (e.g., class labels).\n",
    "   - They are easy to interpret and visualize, making them useful for understanding the decision-making process.\n",
    "\n",
    "3. **Random Forest Classifier**:\n",
    "   - Random Forest Classifier is an ensemble learning method based on decision trees.\n",
    "   - It constructs multiple decision trees during training and outputs the mode of the classes (classification) based on the predictions of the individual trees.\n",
    "   - Random Forest introduces randomness in the tree-building process by using bootstrap samples of the training data and random subsets of features at each node split.\n",
    "   - It is robust to overfitting and noise and typically provides higher accuracy compared to individual decision trees.\n",
    "\n",
    "4. **K-Nearest Neighbors Classifier (KNN)**:\n",
    "   - KNN is a simple and intuitive supervised learning algorithm used for classification tasks.\n",
    "   - It classifies new data points based on the majority class of their nearest neighbors in the feature space.\n",
    "   - The \"k\" in KNN represents the number of nearest neighbors considered for classification.\n",
    "   - KNN does not learn explicit models but rather memorizes the training data, making it computationally inexpensive during training but potentially slow during prediction for large datasets.\n",
    "\n",
    "Each of these classifiers has its own strengths and weaknesses, and the choice of algorithm depends on factors such as the nature of the data, the complexity of the classification task, and computational considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, here's a summary of each classifier along with sample syntax for training and prediction:\n",
    "\n",
    "1. **Support Vector Classifier (SVC)**:\n",
    "   ```python\n",
    "   from sklearn.svm import SVC\n",
    "\n",
    "   # Create SVC classifier object\n",
    "   svc_classifier = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "   # Train the classifier\n",
    "   svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "   # Make predictions\n",
    "   y_pred_svc = svc_classifier.predict(X_test)\n",
    "   ```\n",
    "\n",
    "2. **Decision Tree Classifier**:\n",
    "   ```python\n",
    "   from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "   # Create Decision Tree classifier object\n",
    "   dt_classifier = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "   # Train the classifier\n",
    "   dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "   # Make predictions\n",
    "   y_pred_dt = dt_classifier.predict(X_test)\n",
    "   ```\n",
    "\n",
    "3. **Random Forest Classifier**:\n",
    "   ```python\n",
    "   from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "   # Create Random Forest classifier object\n",
    "   rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=2)\n",
    "\n",
    "   # Train the classifier\n",
    "   rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "   # Make predictions\n",
    "   y_pred_rf = rf_classifier.predict(X_test)\n",
    "   ```\n",
    "\n",
    "4. **K-Nearest Neighbors Classifier (KNN)**:\n",
    "   ```python\n",
    "   from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "   # Create KNN classifier object\n",
    "   knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "   # Train the classifier\n",
    "   knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "   # Make predictions\n",
    "   y_pred_knn = knn_classifier.predict(X_test)\n",
    "   ```\n",
    "\n",
    "These are the basic syntax examples for each classifier using scikit-learn library in Python. Make sure to replace `X_train`, `y_train`, `X_test`, and `y_test` with your actual training and testing data. Additionally, adjust the hyperparameters according to your specific problem and dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
